{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torch import nn\n",
    "from torch.utils import data\n",
    "from d2l import torch as d2l\n",
    "\n",
    "\n",
    "class SpaceAtt(nn.Module):\n",
    "    def __init__(self, inplanes):\n",
    "        super(SpaceAtt, self).__init__()\n",
    "        self.inplanes = inplanes\n",
    "        self.conv_mask = nn.Conv2d(inplanes, 1, kernel_size=1)\n",
    "        self.softmax = nn.Softmax(dim=2)\n",
    "        self.conv_first = nn.Conv2d(inplanes, inplanes, kernel_size=1)\n",
    "        self.bn = nn.BatchNorm2d(1, eps=1e-5, momentum=0.01, affine=True)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch, channel, height, width = x.size()\n",
    "        input_x = x\n",
    "        first_x = self.conv_first(input_x)\n",
    "        first_x = first_x.view(batch, channel, height * width)\n",
    "        first_x = first_x.unsqueeze(1)\n",
    "        # [N, 1, H*W, C]\n",
    "        first_x = first_x.permute(0, 1, 3, 2).contiguous()\n",
    "        # [N, C, H * W]\n",
    "        input_x = input_x.view(batch, channel, height * width)\n",
    "        # [N, 1, C, H * W]\n",
    "        input_x = input_x.unsqueeze(1)\n",
    "        # [N, 1, H*W, H*W]\n",
    "        second_x = torch.matmul(first_x, input_x)\n",
    "        # [N, 1, H, W]\n",
    "        context_mask = self.conv_mask(x)\n",
    "        # [N, 1, H * W]\n",
    "        context_mask = context_mask.view(batch, 1, height * width)\n",
    "        # [N, 1, H * W]\n",
    "        context_mask = self.softmax(context_mask)\n",
    "        # [N, 1, H * W, 1]\n",
    "        context_mask = context_mask.unsqueeze(-1)\n",
    "        # [N, 1, H * W, 1]\n",
    "        context = torch.matmul(second_x, context_mask)\n",
    "        # [N, 1, H, W]\n",
    "        context = context.view(batch, 1, height, width)\n",
    "        # [N, 1, H, W]\n",
    "        out = self.bn(context)\n",
    "        out = self.sigmoid(out)\n",
    "        return out * x\n",
    "\n",
    "\n",
    "class ChannelAtt(nn.Module):\n",
    "    def __init__(self, k_size=5):\n",
    "        super(ChannelAtt, self).__init__()\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.max_pool = nn.AdaptiveMaxPool2d(1)\n",
    "        self.conv = nn.Conv1d(1, 1, kernel_size=k_size, padding=(k_size - 1) // 2, bias=False)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        avg_out = self.avg_pool(x)\n",
    "        max_out = self.max_pool(x)\n",
    "        avg_out = self.conv(avg_out.squeeze(-1).transpose(-1, -2)).transpose(-1, -2).unsqueeze(-1)\n",
    "        max_out = self.conv(max_out.squeeze(-1).transpose(-1, -2)).transpose(-1, -2).unsqueeze(-1)\n",
    "        out = avg_out + max_out\n",
    "        out = self.sigmoid(out)\n",
    "        return out * x\n",
    "\n",
    "\n",
    "class MixSC(nn.Module):\n",
    "    def __init__(self, inplanes):\n",
    "        super(MixSC, self).__init__()\n",
    "        self.space = SpaceAtt(inplanes)\n",
    "        self.channel = ChannelAtt()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.space(x)\n",
    "        x = self.channel(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, stride=1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.residual_function = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_channels, out_channels * BasicBlock.expansion, kernel_size=3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels * BasicBlock.expansion)\n",
    "        )\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "\n",
    "        if stride != 1 or in_channels != BasicBlock.expansion * out_channels:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_channels, out_channels * BasicBlock.expansion, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(out_channels * BasicBlock.expansion)\n",
    "            )\n",
    "\n",
    "        self.sc_att = MixSC(out_channels * BasicBlock.expansion)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return nn.ReLU(inplace=True)(self.sc_att(self.residual_function(x)) + self.shortcut(x))\n",
    "\n",
    "\n",
    "class BottleNeck(nn.Module):\n",
    "    expansion = 4\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, stride=1):\n",
    "        super().__init__()\n",
    "        self.residual_function = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_channels, out_channels, stride=stride, kernel_size=3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_channels, out_channels * BottleNeck.expansion, kernel_size=1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels * BottleNeck.expansion),\n",
    "        )\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "\n",
    "        if stride != 1 or in_channels != out_channels * BottleNeck.expansion:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_channels, out_channels * BottleNeck.expansion, stride=stride, kernel_size=1, bias=False),\n",
    "                nn.BatchNorm2d(out_channels * BottleNeck.expansion)\n",
    "            )\n",
    "        self.sc_att = MixSC(out_channels * BottleNeck.expansion)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return nn.ReLU(inplace=True)(self.sc_att(self.residual_function(x)) + self.shortcut(x))\n",
    "\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "\n",
    "    def __init__(self, block, num_block, num_classes=100):\n",
    "        super().__init__()\n",
    "\n",
    "        self.in_channels = 64\n",
    "\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, kernel_size=3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True))\n",
    "\n",
    "        self.conv2_x = self._make_layer(block, 64, num_block[0], 1)\n",
    "        self.conv3_x = self._make_layer(block, 128, num_block[1], 2)\n",
    "        self.conv4_x = self._make_layer(block, 256, num_block[2], 2)\n",
    "        self.conv5_x = self._make_layer(block, 512, num_block[3], 2)\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.fc = nn.Linear(512 * block.expansion, num_classes)\n",
    "\n",
    "    def _make_layer(self, block, out_channels, num_blocks, stride):\n",
    "        strides = [stride] + [1] * (num_blocks - 1)\n",
    "        layers = []\n",
    "        for stride in strides:\n",
    "            layers.append(block(self.in_channels, out_channels, stride))\n",
    "            self.in_channels = out_channels * block.expansion\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        output = self.conv1(x)\n",
    "        output = self.conv2_x(output)\n",
    "        output = self.conv3_x(output)\n",
    "        output = self.conv4_x(output)\n",
    "        output = self.conv5_x(output)\n",
    "        output = self.avg_pool(output)\n",
    "        output = output.view(output.size(0), -1)\n",
    "        output = self.fc(output)\n",
    "\n",
    "        return output\n",
    "\n",
    "\n",
    "def resnet18_sc():\n",
    "    return ResNet(BasicBlock, [2, 2, 2, 2])\n",
    "\n",
    "\n",
    "def resnet34_sc():\n",
    "    return ResNet(BasicBlock, [3, 4, 6, 3])\n",
    "\n",
    "\n",
    "def resnet50_sc():\n",
    "    return ResNet(BottleNeck, [3, 4, 6, 3])\n",
    "\n",
    "\n",
    "def resnet101_sc():\n",
    "    return ResNet(BottleNeck, [3, 4, 23, 3])\n",
    "\n",
    "\n",
    "def resnet152_sc():\n",
    "    return ResNet(BottleNeck, [3, 8, 36, 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_cifar100(batch_size=128):\n",
    "    transform_train = torchvision.transforms.Compose([\n",
    "        # torchvision.transforms.Resize(256),\n",
    "        # torchvision.transforms.CenterCrop(224),\n",
    "        torchvision.transforms.Resize(40),\n",
    "        torchvision.transforms.RandomResizedCrop(32, scale=(0.64, 1.0),\n",
    "                                                 ratio=(1.0, 1.0)),\n",
    "        torchvision.transforms.RandomHorizontalFlip(),\n",
    "        torchvision.transforms.ToTensor(),\n",
    "        torchvision.transforms.Normalize([0.4914, 0.4822, 0.4465],\n",
    "                                         [0.2023, 0.1994, 0.2010])])\n",
    "\n",
    "    transform_test = torchvision.transforms.Compose([\n",
    "        # torchvision.transforms.Resize(256),\n",
    "        # torchvision.transforms.CenterCrop(224),\n",
    "        torchvision.transforms.ToTensor(),\n",
    "        torchvision.transforms.Normalize([0.4914, 0.4822, 0.4465],\n",
    "                                         [0.2023, 0.1994, 0.2010])])\n",
    "\n",
    "    trainset = torchvision.datasets.CIFAR100(root='./data', train=True,\n",
    "                                             download=True, transform=transform_train)\n",
    "    trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n",
    "                                              shuffle=True, num_workers=0)\n",
    "\n",
    "    testset = torchvision.datasets.CIFAR100(root='./data', train=False,\n",
    "                                            download=True, transform=transform_test)\n",
    "    testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size,\n",
    "                                             shuffle=False, num_workers=0)\n",
    "    return trainloader, testloader\n",
    "\n",
    "\n",
    "def train_batch(net, X, y, loss, trainer, devices):\n",
    "    X = X.to(devices)\n",
    "    y = y.to(devices)\n",
    "    net.train()\n",
    "    trainer.zero_grad()\n",
    "    pred = net(X)\n",
    "    l = loss(pred, y)\n",
    "    l.sum().backward()\n",
    "    trainer.step()\n",
    "    train_loss_sum = l.sum()\n",
    "    train_acc_sum = d2l.accuracy(pred, y)\n",
    "    return train_loss_sum, train_acc_sum\n",
    "\n",
    "\n",
    "def train(net, train_iter, valid_iter, num_epochs, loss, trainer, lr_period,\n",
    "          lr_decay, use_sl=True, device=d2l.try_gpu()):\n",
    "    if use_sl:\n",
    "        scheduler = torch.optim.lr_scheduler.StepLR(trainer, lr_period, lr_decay)\n",
    "    num_batches, timer = len(train_iter), d2l.Timer()\n",
    "    legend = ['train loss', 'train acc']\n",
    "    if valid_iter is not None:\n",
    "        legend.append('valid acc')\n",
    "    animator = d2l.Animator(xlabel='epoch', xlim=[1, num_epochs],\n",
    "                            legend=legend)\n",
    "    net = net.to(device)\n",
    "    for epoch in range(num_epochs):\n",
    "        net.train()\n",
    "        metric = d2l.Accumulator(3)\n",
    "        for i, (features, labels) in enumerate(train_iter):\n",
    "            timer.start()\n",
    "            l, acc = train_batch(net, features, labels, loss,\n",
    "                                 trainer, device)\n",
    "            metric.add(l, acc, labels.shape[0])\n",
    "            timer.stop()\n",
    "            if (i + 1) % (num_batches // 5) == 0 or i == num_batches - 1:\n",
    "                animator.add(\n",
    "                    epoch + (i + 1) / num_batches,\n",
    "                    (metric[0] / metric[2], metric[1] / metric[2], None))\n",
    "        if valid_iter is not None:\n",
    "            valid_acc = evaluate_accuracy(net, valid_iter)\n",
    "            animator.add(epoch + 1, (None, None, valid_acc))\n",
    "        if use_sl:\n",
    "            scheduler.step()\n",
    "    measures = (f'train loss {metric[0] / metric[2]:.3f}, '\n",
    "                f'train acc {metric[1] / metric[2]:.3f}')\n",
    "    if valid_iter is not None:\n",
    "        measures += f', valid acc {valid_acc:.3f}'\n",
    "    print(measures + f'\\n{metric[2] * num_epochs / timer.sum():.1f}'\n",
    "                     f' examples/sec on {str(device)}')\n",
    "\n",
    "\n",
    "def evaluate_accuracy(net, data_iter, device=None):\n",
    "    if isinstance(net, nn.Module):\n",
    "        net.eval()\n",
    "        if not device:\n",
    "            device = next(iter(net.parameters())).device\n",
    "    metric = d2l.Accumulator(2)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X, y in data_iter:\n",
    "            X = X.to(device)\n",
    "            y = y.to(device)\n",
    "            metric.add(d2l.accuracy(net(X), y), d2l.size(y))\n",
    "    return metric[0] / metric[1]\n",
    "\n",
    "\n",
    "def train_fine_tuning(net, learning_rate, lr_period, lr_decay, train_iter, valid_iter, loss, num_epochs=5,\n",
    "                      param_group=True):\n",
    "    if param_group:\n",
    "        params_1x = [param for name, param in net.named_parameters()\n",
    "                     if name not in [\"fc.weight\", \"fc.bias\"]]\n",
    "        trainer = torch.optim.SGD([{'params': params_1x},\n",
    "                                   {'params': net.fc.parameters(),\n",
    "                                    'lr': learning_rate * 10}],\n",
    "                                  lr=learning_rate, weight_decay=0.001)\n",
    "    else:\n",
    "        trainer = torch.optim.SGD(net.parameters(), lr=learning_rate, weight_decay=0.001)\n",
    "    train(net, train_iter, valid_iter, num_epochs, loss, trainer, lr_period, lr_decay)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = resnet50_sc()\n",
    "devices = d2l.try_gpu()\n",
    "num_epochs = 80\n",
    "lr = 2e-4\n",
    "wd = 5e-4\n",
    "lr_period = 40\n",
    "lr_decay = 0.1\n",
    "trainloader, validloader = load_data_cifar100()\n",
    "loss = nn.CrossEntropyLoss(reduction=\"none\")\n",
    "trainer = torch.optim.SGD(net.parameters(), lr=lr, momentum=0.9, weight_decay=wd)\n",
    "train(net, trainloader, validloader, num_epochs, loss, trainer, lr_period, lr_decay)\n",
    "torch.save(net.state_dict(), \"mixsc.pth\", _use_new_zipfile_serialization=True)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
